{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Session Chatbot\n",
    "This is the hands-on session accompanying the workshop on LangChain fundamentals. This is inspired by the more extensive LangChain Cookbook Part 1.\n",
    "\n",
    "Copyright (c) 2023 Michael Neumayr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up the Colab in your drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load this Colab from Github\n",
    "- Run the first cell to install all required packages (this takes a moment)\n",
    "- During installation jump to section \"Set OpenAI API Key\" and put the key we provide you instead of \"PUT_YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages; this may take some minutes; ignore dependency warnings it should work anyway\n",
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install pypdf\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the workshop github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelnoi/venture_labs_build.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd venture_labs_build\n",
    "!git checkout only_static_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY', 'PUT_YOUR_KEY_HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Interactive Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Remember the list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Âú®ÊÖïÂ∞ºÈªëÁöÑÂï§ÈÖíËäÇ‰∏äÔºå‰∫∫‰ª¨ÈÄöÂ∏∏Á©øÁùÄ‰º†ÁªüÁöÑÂ∑¥‰ºêÂà©‰∫öÊúçË£Ö„ÄÇÁî∑ÊÄßÂèØ‰ª•Á©øÁùÄ‚Äúlederhosen‚ÄùÔºàÁöÆÁü≠Ë£§ÔºâÊê≠ÈÖç‰∏Ä‰ª∂ÊñπÊ†ºË°¨Ë°´„ÄÅÈïøË¢úÂíåÁöÆÈûã„ÄÇÂ•≥ÊÄßÂàôÂèØ‰ª•ÈÄâÊã©‚Äúdirndl‚ÄùÔºà‰∏ÄÁßç‰º†ÁªüÁöÑËøûË°£Ë£ôÔºâÔºåÈÄöÂ∏∏ÈÖçÊúâÂõ¥Ë£ô„ÄÅË•üÈ•∞ÂíåËïæ‰∏ù„ÄÇÊ≠§Â§ñÔºåËøòÂèØ‰ª•Êà¥‰∏ä‰∏ÄÈ°∂È∏≠ËàåÂ∏ΩÊàñËÄÖ‰º†ÁªüÁöÑÂ∑¥‰ºêÂà©‰∫öÂ∏ΩÂ≠ê„ÄÇËøô‰∫õ‰º†ÁªüÊúçË£ÖÂèØ‰ª•Âú®ÊÖïÂ∞ºÈªëÁöÑ‰∏ìÂçñÂ∫óÊàñËÄÖÂú®Á∫øÂïÜÂ∫óË¥≠‰π∞Âà∞„ÄÇÂ¶ÇÊûú‰Ω†‰∏çÊâìÁÆóÁ©ø‰º†ÁªüÊúçË£ÖÔºåÈÇ£‰πà‰πüÂèØ‰ª•Á©øÁùÄËàíÈÄÇÁöÑ‰ºëÈó≤Ë£ÖÊàñËÄÖÊôöÁ§ºÊúçÂèÇÂä†Âï§ÈÖíËäÇ„ÄÇ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Answer in Chinese.\"),\n",
    "        HumanMessage(content=\"When is the Oktoberfest in Munich usually?\"),\n",
    "        AIMessage(content='The Oktoberfest in Munich usually begins in late September and lasts for 16-18 days, ending on the first Sunday in October or on October 3rd, German Unity Day, if it falls on a Monday.'),\n",
    "        HumanMessage(content=\"And do you have recommendattions what to wear?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a proper interactive chatbot that stores the messages dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conversation chain for a chatbot\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "    üìé <b>The ConversationChain includes</b>\n",
    "    <ol>\n",
    "        <li>a pre-defined prompt template for a conversation with the LLM. The template is filled with the user input and the chat history. See the template below. This saves you the prompt engineering part for a good conversation. If you like more flexibility, however, you can also use your own prompt template.</li>\n",
    "        <li>per default a memory that stores the conversation history and append it to the prompt.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "chain = ConversationChain(llm=chat) #, verbose=True)\n",
    "\n",
    "exit_conditions = (\"quit\", \"exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is empty at the beginning. The conversation chain will fill it automatically with the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chain.memory.buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember prompt templates. Here, the user input is automatically prefixed by the \"Human: \" keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a loop for a conversation with the LLM. The loop should ask the user for input, send the input to the LLM, and print the response. The loop should stop if the user enters \"quit\" or \"exit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Hello! I'm an AI, so I don't have emotions, but I'm here and ready to help you with any questions or topics you'd like to discuss. How can I assist you today?\n",
      "\n",
      "\n",
      "AI: Sure! What genre or type of books are you interested in?\n",
      "\n",
      "\n",
      "AI: If you're not sure, I can provide recommendations from a variety of genres. Some popular options include:\n",
      "\n",
      "- \"1984\" by George Orwell: This classic dystopian novel explores themes of government surveillance and the dangers of totalitarianism.\n",
      "- \"To Kill a Mockingbird\" by Harper Lee: This Pulitzer Prize-winning novel tackles issues of racial injustice and morality in the American South.\n",
      "- \"Pride and Prejudice\" by Jane Austen: A beloved romance novel set in 19th-century England, it explores themes of societal expectations and personal growth.\n",
      "- \"The Great Gatsby\" by F. Scott Fitzgerald: Set in the Jazz Age, this novel delves into themes of wealth, love, and the American Dream.\n",
      "- \"The Lord of the Rings\" by J.R.R. Tolkien: A fantasy epic filled with adventure, friendship, and the battle between good and evil.\n",
      "\n",
      "Let me know if any of these catch your interest, or if you have any specific preferences!\n",
      "\n",
      "\n",
      "AI: Great! Here are some non-fiction book recommendations for you:\n",
      "\n",
      "- \"Sapiens: A Brief History of Humankind\" by Yuval Noah Harari: This book offers a thought-provoking exploration of the history and impact of Homo sapiens on the world.\n",
      "- \"Becoming\" by Michelle Obama: In this memoir, Michelle Obama shares her personal journey, from her childhood on the South Side of Chicago to her time as First Lady of the United States.\n",
      "- \"The Immortal Life of Henrietta Lacks\" by Rebecca Skloot: This book tells the true story of Henrietta Lacks, whose cells were unknowingly used for scientific research and became crucial to medical advancements.\n",
      "- \"Educated\" by Tara Westover: A memoir about a woman who grows up in a strict and abusive household in rural Idaho but eventually escapes to pursue an education.\n",
      "- \"Sapiens: A Graphic History\" by Yuval Noah Harari and David Vandermeulen: This graphic novel adaptation of \"Sapiens\" provides a visually engaging way to explore the history of humankind.\n",
      "\n",
      "I hope you find these recommendations interesting! Let me know if you'd like more suggestions or if you have any specific topics in mind.\n",
      "\n",
      "\n",
      "AI: If you have any other questions or need assistance with anything else, feel free to let me know! I'm here to help.\n",
      "\n",
      "\n",
      "AI: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"Human: \")\n",
    "    if query in exit_conditions:\n",
    "        print()\n",
    "        print(\"AI: Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = chain.predict(input=query)\n",
    "        print()\n",
    "        print(f\"AI: {response}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the memory again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color: #151E35; color: #A450E6\">\n",
    "    üéØ <b>TODO</b>\n",
    "  <p>Now check the memory of the chain to see if it stored your conversation correctly. You can get a more readible format if you use buffer_as_messages.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hey, how are you?\n",
      "AI: Hello! I'm an AI, so I don't have emotions, but I'm here and ready to help you with any questions or topics you'd like to discuss. How can I assist you today?\n",
      "Human: give me some book recommendations\n",
      "AI: Sure! What genre or type of books are you interested in?\n",
      "Human: \n",
      "AI: If you're not sure, I can provide recommendations from a variety of genres. Some popular options include:\n",
      "\n",
      "- \"1984\" by George Orwell: This classic dystopian novel explores themes of government surveillance and the dangers of totalitarianism.\n",
      "- \"To Kill a Mockingbird\" by Harper Lee: This Pulitzer Prize-winning novel tackles issues of racial injustice and morality in the American South.\n",
      "- \"Pride and Prejudice\" by Jane Austen: A beloved romance novel set in 19th-century England, it explores themes of societal expectations and personal growth.\n",
      "- \"The Great Gatsby\" by F. Scott Fitzgerald: Set in the Jazz Age, this novel delves into themes of wealth, love, and the American Dream.\n",
      "- \"The Lord of the Rings\" by J.R.R. Tolkien: A fantasy epic filled with adventure, friendship, and the battle between good and evil.\n",
      "\n",
      "Let me know if any of these catch your interest, or if you have any specific preferences!\n",
      "Human: non-fiction\n",
      "AI: Great! Here are some non-fiction book recommendations for you:\n",
      "\n",
      "- \"Sapiens: A Brief History of Humankind\" by Yuval Noah Harari: This book offers a thought-provoking exploration of the history and impact of Homo sapiens on the world.\n",
      "- \"Becoming\" by Michelle Obama: In this memoir, Michelle Obama shares her personal journey, from her childhood on the South Side of Chicago to her time as First Lady of the United States.\n",
      "- \"The Immortal Life of Henrietta Lacks\" by Rebecca Skloot: This book tells the true story of Henrietta Lacks, whose cells were unknowingly used for scientific research and became crucial to medical advancements.\n",
      "- \"Educated\" by Tara Westover: A memoir about a woman who grows up in a strict and abusive household in rural Idaho but eventually escapes to pursue an education.\n",
      "- \"Sapiens: A Graphic History\" by Yuval Noah Harari and David Vandermeulen: This graphic novel adaptation of \"Sapiens\" provides a visually engaging way to explore the history of humankind.\n",
      "\n",
      "I hope you find these recommendations interesting! Let me know if you'd like more suggestions or if you have any specific topics in mind.\n",
      "Human: \n",
      "AI: If you have any other questions or need assistance with anything else, feel free to let me know! I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "### TODO: print the stored memory of your conversation\n",
    "\n",
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Longer conversations\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "    üìé <b>Hitting the token limit</b>\n",
    "    <p>Like with handling large documents, you can also hit the token limit with conversations. But that is not just the case when your input is too large but also when the conversation is too long. This happens because you add the whole history to every prompt you make, so the history grows linearly with the number of interactions. This is bad for long conversations and there are multiple ways to fix this.</p>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory, ConversationBufferWindowMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "chain = ConversationChain(llm=chat) #, verbose=True)\n",
    "\n",
    "exit_conditions = (\"quit\", \"exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Cut the conversation off after a specified limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest version. You can just cut off the conversation after a specified number of interactions. This won't run into any token limit problems but it will also not be able to handle long conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color: #151E35; color: #A450E6\">\n",
    "    üéØ <b>TODO</b>\n",
    "  <p>The keyword <code style=\"color:#A450E6\">k</code> is the window size, i.e. how many last interaction will be stored. Play around with k and tell the model something in a conversation and and ask it at some later point again.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "\tllm=chat,\n",
    "\tmemory=ConversationBufferWindowMemory(k=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Yes, that's correct! Grass is typically green due to the presence of chlorophyll, which is responsible for absorbing sunlight and converting it into energy through the process of photosynthesis. The green color of grass is a result of chlorophyll's ability to absorb red and blue light while reflecting green light. This natural pigment is found in the chloroplasts of grass cells, giving it its vibrant green appearance.\n",
      "\n",
      "\n",
      "AI: Actually, the color of water can vary depending on various factors such as the presence of impurities, the depth of the water, and the angle at which light hits it. In many cases, water appears to be blue due to selective absorption and scattering of light. Water molecules absorb longer-wavelength colors such as red and reflect shorter-wavelength colors like blue. This is why bodies of water, such as oceans and lakes, often appear blue or greenish-blue. However, it's important to note that clear water can also appear colorless or take on the color of its surroundings.\n",
      "\n",
      "\n",
      "AI: You said \"the water is blue\".\n",
      "\n",
      "\n",
      "AI: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"Human: \")\n",
    "    if query in exit_conditions:\n",
    "        print()\n",
    "        print(\"AI: Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = conversation.predict(input=query)\n",
    "        print()\n",
    "        print(f\"AI: {response}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Summarize the conversation and append the summary to the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method is to summarize the conversation so far and append that instead of whole raw history to every prompt. This uses more tokens at the beginning but scales better for larger conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color: #151E35; color: #A450E6\">\n",
    "    üéØ <b>TODO</b>\n",
    "  <p>Now utilize <code style=\"color:#A450E6\">ConversationSummaryMemory</code> instead of the window memory. You can set it up in the same way, the only difference is, that the summary memory needs an llm (for the summary) as input instead of a window size. The keywork is <code style=\"color:#A450E6\">llm=</code>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: set up the conversation chain with a ConversationSummaryMemory here\n",
    "\n",
    "conversation = ConversationChain(\n",
    "\tllm=chat,\n",
    "\tmemory=ConversationSummaryMemory(llm=chat)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it out and print intermediate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: I'm doing well, thank you! I'm an AI designed to assist with various tasks and provide information. How can I help you today?\n",
      "\n",
      "\n",
      "AI: Sure! I can definitely help you with that. Here's a simple recipe for chicken curry:\n",
      "\n",
      "Ingredients:\n",
      "- 500g boneless chicken, cut into pieces\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 large onion, finely chopped\n",
      "- 3 cloves of garlic, minced\n",
      "- 1 tablespoon ginger, grated\n",
      "- 2 teaspoons curry powder\n",
      "- 1 teaspoon turmeric powder\n",
      "- 1 teaspoon cumin powder\n",
      "- 1 teaspoon coriander powder\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- 1 cup coconut milk\n",
      "- 1 cup chicken broth\n",
      "- Salt, to taste\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "- Rice or naan bread (for serving)\n",
      "\n",
      "Instructions:\n",
      "1. Heat the vegetable oil in a large pan over medium heat. Add the chopped onion and saut√© until it becomes translucent.\n",
      "2. Add the minced garlic and grated ginger, and cook for another minute until fragrant.\n",
      "3. Add the chicken pieces to the pan and cook until they are lightly browned on all sides.\n",
      "4. In a small bowl, mix together the curry powder, turmeric powder, cumin powder, coriander powder, and chili powder. Add this spice mixture to the pan and stir well to coat the chicken evenly.\n",
      "5. Pour in the coconut milk and chicken broth. Stir everything together and bring the mixture to a simmer.\n",
      "6. Reduce the heat to low, cover the pan, and let the curry simmer for about 20-25 minutes, or until the chicken is cooked through and tender. Stir occasionally to prevent sticking.\n",
      "7. Taste the curry and adjust the seasoning with salt if needed.\n",
      "8. Once the curry is done, garnish with freshly chopped cilantro.\n",
      "9. Serve hot with rice or naan bread.\n",
      "\n",
      "Enjoy your homemade chicken curry! Let me know if you need any further assistance or if there's anything else I can help with.\n",
      "\n",
      "\n",
      "AI: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"Human: \")\n",
    "    if query in exit_conditions:\n",
    "        print()\n",
    "        print(\"AI: Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = conversation.predict(input=query)\n",
    "        print()\n",
    "        print(f\"AI: {response}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human greets the AI and asks how it is doing. The AI responds that it is doing well and explains its purpose as an AI designed to assist with tasks and provide information. It asks the human how it can help. The human asks the AI for a recipe for chicken curry. The AI provides a detailed recipe for chicken curry, including a list of ingredients and step-by-step instructions. It also offers further assistance if needed.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) Compare token usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also combinations of these techniques and more advanced methods, see below how the token usage scales with the number of interactions.\n",
    "\n",
    "<img src=\"static/token_usage.png\" width=\"1000\"/>\n",
    "\n",
    "Source: https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Streaming output, the real ChatGPT experience\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "    üìé <b>Streaming output</b>\n",
    "    <p>Previously, you always had to wait for the full answer and only then was the result printed. Now, we want to have the real ChatGPT experience and also stream the output token by token as soon at is ready.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same conversation model, we just change how we call the chain. For streaming the outputs, we leverage the <code style=\"color:gray\">streaming</code> keyword. This will return a generator that yields the tokens as they are generated. We can then print them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], openai_api_key=openai_api_key)\n",
    "chain = ConversationChain(llm=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm where words and data collide,\n",
      "A poem of Natural Language Processing resides.\n",
      "Where algorithms dance with linguistic flair,\n",
      "Unraveling the mysteries of language, so rare.\n",
      "\n",
      "In the vast expanse of textual verse,\n",
      "NLP unlocks meaning, like a universe.\n",
      "Sentences and phrases, in abundance they flow,\n",
      "Through the digital realm, where insights bestow.\n",
      "\n",
      "With every line, a token, carefully dissected,\n",
      "Syntax and semantics, meticulously connected.\n",
      "Parsing through structure, with computational might,\n",
      "NLP unveils the beauty hidden in plain sight.\n",
      "\n",
      "From sentiment analysis to sentiment classification,\n",
      "NLP deciphers emotions, with precision and diction.\n",
      "It understands the context, the subtlety, and tone,\n",
      "Revealing the depths of what words can condone.\n",
      "\n",
      "Topic modeling, it weaves a tapestry of themes,\n",
      "Extracting knowledge from vast textual streams.\n",
      "Clustering words and ideas, in a harmonious blend,\n",
      "NLP brings coherence, where chaos may transcend.\n",
      "\n",
      "Machine translation, across languages it roams,\n",
      "Breaking down barriers, connecting distant homes.\n",
      "Transcending borders, it bridges the divide,\n",
      "Uniting cultures, with words as a guide.\n",
      "\n",
      "But NLP is more than just lines of code,\n",
      "It's a testament to human language, bestowed.\n",
      "For in this realm, where algorithms strive,\n",
      "It's the human touch that makes NLP come alive.\n",
      "\n",
      "So let us celebrate this wondrous creation,\n",
      "That empowers communication, across every nation.\n",
      "For in the realm where words and data converge,\n",
      "NLP shines bright, as a linguistic surge."
     ]
    }
   ],
   "source": [
    "answer = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"Give me a poem about natural language processing\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chatbot with streaming output\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #A450E6\">\n",
    "    üéØ <b>TODO</b>\n",
    "  <p>Put together all the parts from above to create an interactive chatbot that has some memory and streams the output like you know from ChatGPT.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: put your imports here\n",
    "\n",
    "# use from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: put your initializations here\n",
    "\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], openai_api_key=openai_api_key)\n",
    "conversation = ConversationChain(llm=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: put your loop here, you don't need to add different arguments to predict for streaming to work as it is already set up in the chat model\n",
    "\n",
    "while True:\n",
    "    query = input(\"Human: \")\n",
    "    if query in exit_conditions:\n",
    "        print()\n",
    "        print(\"AI: Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = conversation.predict(input=query)\n",
    "        print()\n",
    "        print(f\"AI: {response}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documentation: https://python.langchain.com/docs/get_started/introduction\n",
    "- Really comprehensive tutorials: https://github.com/gkamradt/langchain-tutorials\n",
    "- Deep dive conversational memory: https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
